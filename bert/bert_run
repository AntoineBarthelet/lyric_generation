python run_classifier.py --task_name=MRPC --do_train=true --do_eval=true --data_dir=glue --vocab_file=uncased_L-12_H-768_A-12\vocab.txt --bert_config_file=uncased_L-12_H-768_A-12\bert_config.json --init_checkpoint=uncased_L-12_H-768_A-12\bert_model.ckpt --max_seq_length=128 --train_batch_size=8 --learning_rate=2e-5 --num_train_epochs=3.0 --output_dir=tmp\mrpc_output\



python -m src.main -gpu 0 -maxlen 30 -batch_size 8 -lr 2e-5 -max_eps 3


python ./examples/run_language_modeling.py --train_data_file DE_lyrics.txt --output_dir tmp\bd --model_type bert --line_by_line --model_name_or_path bert-base-uncased --do_train --per_gpu_train_batch_size=6 --mlm --num_train_epochs=5 --overwrite_output_dir






--model_type bert --model_name_or_path bert-base-uncased --task_name MRPC --do_train --do_eval --do_lower_case --data_dir glue --max_seq_length 128 --per_gpu_eval_batch_size=6   --per_gpu_train_batch_size=6   --learning_rate 2e-7 --num_train_epochs 1 --output_dir tmp\bd

python subword_builder.py --corpus_filepattern "all_songs.txt" --output_filename vocab.txt --min_count 5

python bert/create_pretraining_data.py --input_file=proc_dataset.txt --output_file=pretraining_data/{}.tfrecord --vocab_file=vocab.txt --do_lower_case=True --max_predictions_per_seq=20 --max_seq_length=128 --masked_lm_prob=0.15 --random_seed=34 --dupe_factor=5

onlyfiles = [f for f in listdir('bd_bert') if isfile(join(mypath, f))]